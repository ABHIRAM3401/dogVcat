{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dont forgot to use the version tags in git after changing the code . we have to use \n",
    "those v tags once we are done with entire code and making some minor updates \n",
    "we need to push those tags separately.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT link : https://chatgpt.com/c/67c31846-7cec-8003-9bae-1f349c1f37c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow → Main framework for deep learning.\n",
    "\n",
    "🔹 keras → High-level API inside TensorFlow for building neural networks.\n",
    "\n",
    "🔹 Sequential → A linear stack of layers for CNN.\n",
    "\n",
    "🔹 Conv2D, MaxPooling2D, Flatten, Dense, Dropout → Different layers for building CNN.\n",
    "\n",
    "🔹 ImageDataGenerator → Helps load and preprocess images.\n",
    "\n",
    "🔹 numpy → Used for numerical operations.\n",
    "\n",
    "🔹 matplotlib.pyplot → Helps plot training results.\n",
    "\n",
    "🔹 cv2 → OpenCV for image processing (resizing, conversion).\n",
    "\n",
    "🔹 os → Helps in handling file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf  \n",
    "from tensorflow import keras  \n",
    "from tensorflow.keras.models import Sequential  \n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout  , BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in \"tensorflow.keras.preprocessing.image\", each word separated by a dot (.) represents a package, module, or submodule in Python.\n",
    "tensorflow → The main TensorFlow package.\n",
    "\n",
    "keras → A submodule inside TensorFlow that provides deep learning APIs.\n",
    "\n",
    "preprocessing → A submodule inside Keras that contains tools for data preprocessing.\n",
    "\n",
    "image → A module inside preprocessing that specifically deals with image data (e.g., ImageDataGenerator, load_img, img_to_array).\n",
    "\n",
    "\n",
    "\n",
    "REASON FOR TARGET SIZE\n",
    "every image in dataset is 512x512\n",
    "\n",
    "Why Does 512x512 Require More GPU?\n",
    "More Pixels → More Computation\n",
    "\n",
    "A 512x512 image has 262,144 pixels, whereas a 224x224 image has only 50,176 pixels.\n",
    "That’s 5x more pixels, meaning 5x more computation per image!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 2 classes.\n",
      "Found 200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# https://chatgpt.com/c/67d037a5-f280-8003-a91b-60bba6b2af07\n",
    "\n",
    "# need to learn about mixed precision \n",
    "\n",
    "# Define ImageDataGenerator with Augmentation for Training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  \n",
    "    rotation_range=40,       # Rotate images randomly by 40 degrees\n",
    "    width_shift_range=0.2,   # Shift width randomly\n",
    "    height_shift_range=0.2,  # Shift height randomly\n",
    "    shear_range=0.2,         # Shear transformation\n",
    "    zoom_range=0.2,          # Zoom in/out\n",
    "    horizontal_flip=True,    # Flip images horizontally\n",
    "    fill_mode='nearest',     # Fill missing pixels\n",
    "    validation_split=0.2     # 80-20 train-validation split\n",
    ")\n",
    "\n",
    "# ImageDataGenerator for Validation (No Augmentation)\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  \n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load Training Images with Augmentation\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    \"animals\",  \n",
    "    target_size=(224, 224),  \n",
    "    batch_size=32,  \n",
    "    class_mode=\"binary\",  \n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "# Load Validation Images (No Augmentation)\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    \"animals\",  \n",
    "    target_size=(224, 224),  \n",
    "    batch_size=32,  \n",
    "    class_mode=\"binary\",  \n",
    "    subset=\"validation\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': 0, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 222, 222, 32)     128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 111, 111, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 109, 109, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 54, 54, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 52, 52, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 26, 26, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 86528)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               44302848  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,397,505\n",
      "Trainable params: 44,397,057\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# https://chatgpt.com/c/67dc0951-d250-8003-af27-3128f21b2498\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "\n",
    "    # 1st Convolutional Block\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    BatchNormalization(),  # Normalize activations\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    # 2nd Convolutional Block\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    # 3rd Convolutional Block\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    # Flatten and Fully Connected Layers\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),  # Dropout to prevent overfitting\n",
    "    Dense(1, activation='sigmoid')  # Output Layer (Binary Classification)\n",
    "])\n",
    "\n",
    "# Print Model Summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),  # Adam optimizer with a small learning rate\n",
    "    loss=\"binary_crossentropy\",  # Loss function for binary classification\n",
    "    metrics=[\"accuracy\"]  # Metric to evaluate performance\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# change the folder for every trail\n",
    "\n",
    "# Save a checkpoint every 2 epochs\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    r\"checkpoints/first_trail/model_epoch_{epoch:02d}.h5\",  # Saves model as \"model_epoch_01.h5\", \"model_epoch_02.h5\", etc.\n",
    "    save_freq='epoch',  # Save after every epoch\n",
    "    period=1  # Save every 2 epochs\n",
    ")\n",
    "\n",
    "# Early Stopping (optional, but useful)\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "# Reduce learning rate if validation loss stops improving\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Combine callbacks\n",
    "callbacks = [model_checkpoint, early_stopping, reduce_lr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "25/25 [==============================] - 26s 1s/step - loss: 0.3310 - accuracy: 0.8625 - val_loss: 7.1429 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "25/25 [==============================] - 34s 1s/step - loss: 0.3426 - accuracy: 0.8612 - val_loss: 6.5370 - val_accuracy: 0.5050 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "25/25 [==============================] - 24s 964ms/step - loss: 0.2521 - accuracy: 0.8813 - val_loss: 5.4440 - val_accuracy: 0.5100 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "25/25 [==============================] - 25s 979ms/step - loss: 0.2699 - accuracy: 0.8950 - val_loss: 4.8150 - val_accuracy: 0.5150 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "25/25 [==============================] - 22s 894ms/step - loss: 0.2665 - accuracy: 0.8938 - val_loss: 3.7508 - val_accuracy: 0.5300 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "25/25 [==============================] - 25s 990ms/step - loss: 0.2232 - accuracy: 0.8975 - val_loss: 2.5965 - val_accuracy: 0.5750 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "25/25 [==============================] - 24s 972ms/step - loss: 0.2006 - accuracy: 0.9200 - val_loss: 2.3124 - val_accuracy: 0.5900 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "25/25 [==============================] - 25s 977ms/step - loss: 0.2169 - accuracy: 0.9162 - val_loss: 1.1413 - val_accuracy: 0.6900 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "25/25 [==============================] - 25s 993ms/step - loss: 0.2271 - accuracy: 0.9175 - val_loss: 0.3150 - val_accuracy: 0.8700 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "25/25 [==============================] - 24s 943ms/step - loss: 0.2046 - accuracy: 0.9100 - val_loss: 0.2739 - val_accuracy: 0.8800 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "25/25 [==============================] - 25s 974ms/step - loss: 0.1913 - accuracy: 0.9200 - val_loss: 0.3874 - val_accuracy: 0.8500 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "25/25 [==============================] - 25s 999ms/step - loss: 0.1952 - accuracy: 0.9237 - val_loss: 0.1583 - val_accuracy: 0.9300 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "25/25 [==============================] - 24s 953ms/step - loss: 0.1418 - accuracy: 0.9413 - val_loss: 0.1765 - val_accuracy: 0.9250 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "25/25 [==============================] - 25s 993ms/step - loss: 0.1982 - accuracy: 0.9337 - val_loss: 0.1011 - val_accuracy: 0.9700 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "25/25 [==============================] - 24s 948ms/step - loss: 0.1124 - accuracy: 0.9488 - val_loss: 0.1485 - val_accuracy: 0.9450 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "25/25 [==============================] - 23s 927ms/step - loss: 0.1584 - accuracy: 0.9463 - val_loss: 0.1111 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      "25/25 [==============================] - 23s 934ms/step - loss: 0.1174 - accuracy: 0.9525 - val_loss: 0.1028 - val_accuracy: 0.9700 - lr: 1.0000e-04\n",
      "Epoch 18/30\n",
      "25/25 [==============================] - 24s 941ms/step - loss: 0.1230 - accuracy: 0.9588 - val_loss: 0.0869 - val_accuracy: 0.9800 - lr: 5.0000e-05\n",
      "Epoch 19/30\n",
      "25/25 [==============================] - 24s 963ms/step - loss: 0.1098 - accuracy: 0.9625 - val_loss: 0.0428 - val_accuracy: 0.9850 - lr: 5.0000e-05\n",
      "Epoch 20/30\n",
      "25/25 [==============================] - 24s 951ms/step - loss: 0.0888 - accuracy: 0.9688 - val_loss: 0.0567 - val_accuracy: 0.9850 - lr: 5.0000e-05\n",
      "Epoch 21/30\n",
      "25/25 [==============================] - 24s 960ms/step - loss: 0.1071 - accuracy: 0.9588 - val_loss: 0.0299 - val_accuracy: 0.9850 - lr: 5.0000e-05\n",
      "Epoch 22/30\n",
      "25/25 [==============================] - 24s 935ms/step - loss: 0.0803 - accuracy: 0.9750 - val_loss: 0.0477 - val_accuracy: 0.9850 - lr: 5.0000e-05\n",
      "Epoch 23/30\n",
      "25/25 [==============================] - 21s 852ms/step - loss: 0.0895 - accuracy: 0.9663 - val_loss: 0.0358 - val_accuracy: 0.9850 - lr: 5.0000e-05\n",
      "Epoch 24/30\n",
      "25/25 [==============================] - 21s 826ms/step - loss: 0.1170 - accuracy: 0.9525 - val_loss: 0.0287 - val_accuracy: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 25/30\n",
      "25/25 [==============================] - 21s 838ms/step - loss: 0.0983 - accuracy: 0.9700 - val_loss: 0.0447 - val_accuracy: 0.9750 - lr: 5.0000e-05\n",
      "Epoch 26/30\n",
      "25/25 [==============================] - 21s 833ms/step - loss: 0.0636 - accuracy: 0.9800 - val_loss: 0.0444 - val_accuracy: 0.9750 - lr: 5.0000e-05\n",
      "Epoch 27/30\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.1084 - accuracy: 0.9550 - val_loss: 0.0228 - val_accuracy: 0.9950 - lr: 5.0000e-05\n",
      "Epoch 28/30\n",
      "25/25 [==============================] - 37s 1s/step - loss: 0.0767 - accuracy: 0.9700 - val_loss: 0.0170 - val_accuracy: 1.0000 - lr: 5.0000e-05\n",
      "Epoch 29/30\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.0809 - accuracy: 0.9712 - val_loss: 0.0240 - val_accuracy: 0.9900 - lr: 5.0000e-05\n",
      "Epoch 30/30\n",
      "25/25 [==============================] - 38s 2s/step - loss: 0.0621 - accuracy: 0.9800 - val_loss: 0.0441 - val_accuracy: 0.9750 - lr: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,  \n",
    "    validation_data=validation_generator,  \n",
    "    epochs=30,  # Adjust based on performance\n",
    "    callbacks=callbacks,  # Includes checkpointing, early stopping, etc.\n",
    "    verbose=1  # Shows training progress\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 3s 357ms/step - loss: 0.0441 - accuracy: 0.9750\n",
      "Validation Accuracy: 97.50%\n",
      "Validation Loss: 0.0441\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the validation set\n",
    "val_loss, val_acc = model.evaluate(validation_generator)\n",
    "print(f\"Validation Accuracy: {val_acc * 100:.2f}%\")\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Path to your saved model (adjust the filename if needed)\n",
    "model_path = r\"C:\\Users\\abhir\\Documents\\project\\dog vs cat\\checkpoints\\first_trail\\model_epoch_30.h5\"\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(model_path)\n",
    "\n",
    "print(\"✅ Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Function to preprocess and predict an image\n",
    "def predict_image(img_path, model):\n",
    "    # Load image and resize it to match model input size\n",
    "    img = image.load_img(img_path, target_size=(224, 224))  # Change target_size if needed\n",
    "\n",
    "    # Convert image to array\n",
    "    img_array = image.img_to_array(img)\n",
    "\n",
    "    # Expand dimensions to match model input shape (batch of 1)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # Normalize the image (same as training)\n",
    "    img_array /= 255.0  \n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict(img_array)[0][0]  # Get first value\n",
    "\n",
    "    # Map prediction to class label\n",
    "    if prediction > 0.5:\n",
    "        label = \"Dog 🐶\"\n",
    "    else:\n",
    "        label = \"Cat 🐱\"\n",
    "\n",
    "    print(f\"Predicted class: {label} (Confidence: {prediction:.2f})\")\n",
    "\n",
    "# Test the function with your image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted class: Cat 🐱 (Confidence: 0.07)\n"
     ]
    }
   ],
   "source": [
    "test_image_path = r\"C:\\Users\\abhir\\Downloads\\cat4.jpeg\"\n",
    "predict_image(test_image_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/1 [==============================] - 0s 26ms/step\n",
    "# Predicted class: Dog 🐶 (Confidence: 0.66\n",
    "\n",
    "# the confidece is quite less as i have given an image which has two dogs instead of one , and the model haven't seen those till now an image with two dogs\n",
    "\n",
    "\n",
    "# dog 3 and cat 3 doesn't worked well as they look quite different"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
